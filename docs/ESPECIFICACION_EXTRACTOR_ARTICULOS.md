
# Especificaci√≥n T√©cnica: M√≥dulo de Extracci√≥n de Texto Completo de Art√≠culos

## 1. Visi√≥n General

### 1.1 Objetivo
Desarrollar un m√≥dulo integrado en el proyecto **RSS China News Filter** que descargue y extraiga el texto completo de art√≠culos de noticias desde sus URLs, produciendo contenido limpio y normalizado para an√°lisis posterior.

### 1.2 Alcance
- Procesamiento de ~100-500 noticias/d√≠a
- Extracci√≥n robusta con fallbacks inteligentes
- Integraci√≥n con la arquitectura modular existente
- Soporte para medios espa√±oles (El Pa√≠s, El Mundo, ABC, La Vanguardia, La Raz√≥n)
- Manejo de bloqueos y contenido din√°mico

### 1.3 Principios de Dise√±o
- **Coherencia**: Mantener el estilo arquitect√≥nico del proyecto actual
- **Robustez**: Nunca fallar completamente; degradar gracefully
- **Eficiencia**: Optimizado para volumen medio sin sobrecarga
- **Observabilidad**: Logging detallado y m√©tricas de ejecuci√≥n

---

## 2. Arquitectura del Sistema

### 2.1 Integraci√≥n con Proyecto Existente

```
f:/pautalla/china/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ feeds_list.py           # [EXISTENTE] Carga feeds
‚îÇ   ‚îú‚îÄ‚îÄ downloader.py           # [EXISTENTE] Descarga RSS
‚îÇ   ‚îú‚îÄ‚îÄ parser.py               # [EXISTENTE] Parsea RSS
‚îÇ   ‚îú‚îÄ‚îÄ filtro_china.py         # [EXISTENTE] Filtra keywords
‚îÇ   ‚îú‚îÄ‚îÄ deduplicador.py         # [EXISTENTE] Deduplicaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ almacenamiento.py       # [EXISTENTE] Guarda resultados
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # [EXISTENTE] CLI principal
‚îÇ   ‚îú‚îÄ‚îÄ gui.py                  # [EXISTENTE] GUI Tkinter
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ article_downloader.py   # [NUEVO] Descarga HTML de art√≠culos
‚îÇ   ‚îú‚îÄ‚îÄ article_extractor.py    # [NUEVO] Extrae texto con trafilatura
‚îÇ   ‚îú‚îÄ‚îÄ article_cleaner.py      # [NUEVO] Limpia y normaliza texto
‚îÇ   ‚îú‚îÄ‚îÄ article_enricher.py     # [NUEVO] Detecta idioma y metadatos
‚îÇ   ‚îú‚îÄ‚îÄ article_fallback.py     # [NUEVO] Fallback con Playwright (opcional)
‚îÇ   ‚îú‚îÄ‚îÄ article_processor.py    # [NUEVO] Orquestador principal
‚îÇ   ‚îî‚îÄ‚îÄ main_extractor.py       # [NUEVO] CLI para extracci√≥n
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ feeds.json              # [EXISTENTE]
‚îÇ   ‚îú‚îÄ‚îÄ keywords.json           # [EXISTENTE]
‚îÇ   ‚îî‚îÄ‚îÄ extractor_config.yaml   # [NUEVO] Configuraci√≥n extractor
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ output.jsonl            # [EXISTENTE] Noticias filtradas
‚îÇ   ‚îú‚îÄ‚îÄ output.csv              # [EXISTENTE]
‚îÇ   ‚îú‚îÄ‚îÄ articles_full.jsonl     # [NUEVO] Art√≠culos completos
‚îÇ   ‚îú‚îÄ‚îÄ articles_full.csv       # [NUEVO]
‚îÇ   ‚îî‚îÄ‚îÄ failed_extractions.jsonl # [NUEVO] URLs fallidas
‚îÇ
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îú‚îÄ‚îÄ rss_china.log           # [EXISTENTE]
‚îÇ   ‚îú‚îÄ‚îÄ rss_china_gui.log       # [EXISTENTE]
‚îÇ   ‚îî‚îÄ‚îÄ article_extractor.log   # [NUEVO]
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ ESPECIFICACION_EXTRACTOR_ARTICULOS.md  # [ESTE DOCUMENTO]
‚îÇ   ‚îî‚îÄ‚îÄ ESTRATEGIA_FALLBACK.md  # [NUEVO] Gu√≠a de fallbacks
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_article_extractor/ # [NUEVO] Tests unitarios
‚îÇ       ‚îú‚îÄ‚îÄ test_downloader.py
‚îÇ       ‚îú‚îÄ‚îÄ test_extractor.py
‚îÇ       ‚îú‚îÄ‚îÄ test_cleaner.py
‚îÇ       ‚îî‚îÄ‚îÄ test_integration.py
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt            # [ACTUALIZAR] A√±adir nuevas deps
‚îî‚îÄ‚îÄ README.md                   # [ACTUALIZAR] Documentar nuevo m√≥dulo
```

### 2.2 Flujo de Datos

```
[output.jsonl] ‚Üí article_processor
                      ‚Üì
              article_downloader (HTML)
                      ‚Üì
              article_extractor (trafilatura)
                      ‚Üì
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚Üì                         ‚Üì
   [texto OK]              [texto vac√≠o/corto]
         ‚Üì                         ‚Üì
   article_cleaner         article_fallback (Playwright)
         ‚Üì                         ‚Üì
   article_enricher         article_cleaner
         ‚Üì                         ‚Üì
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
              [articles_full.jsonl]
              [articles_full.csv]
              [failed_extractions.jsonl]
```

---

## 3. M√≥dulos y Responsabilidades

### 3.1 `article_downloader.py`

**Responsabilidad**: Descarga HTML de URLs de art√≠culos con reintentos y rate limiting.

**Funciones principales**:
- `download_article_html(url: str, timeout: int, headers: dict) -> Tuple[str, str, int]`
  - Retorna: `(html_content, final_url, status_code)`
  - Maneja redirecciones autom√°ticas
  - Aplica reintentos con backoff exponencial (tenacity)

- `download_articles_batch(urls: List[str], concurrency: int, delay_per_domain: float) -> List[DownloadResult]`
  - Descarga batch con control de concurrencia
  - Rate limiting por dominio
  - Retorna lista de `DownloadResult` (dataclass)

**Configuraci√≥n**:
```python
DEFAULT_TIMEOUT = 15  # segundos
MAX_RETRIES = 3
BACKOFF_MULTIPLIER = 2
DELAY_BETWEEN_REQUESTS_SAME_DOMAIN = 1.0  # segundos
DEFAULT_USER_AGENT = 'Mozilla/5.0 (compatible; RSSChinaBot-ArticleExtractor/1.0)'
```

**Manejo de errores**:
- `200`: OK, devolver HTML
- `3xx`: Seguir redirecciones (autom√°tico con requests)
- `404, 410`: Error permanente, no reintentar
- `429, 503`: Rate limit / servidor ocupado, reintentar con backoff mayor
- `5xx`: Error temporal, reintentar
- `Timeout`: Reintentar hasta MAX_RETRIES
- `ConnectionError`: Reintentar hasta MAX_RETRIES

**Detecci√≥n de bloqueos**:
- Buscar patrones en HTML: "captcha", "robot", "blocked", "access denied"
- Verificar Content-Type (debe ser text/html)
- Tama√±o sospechoso (< 500 bytes o > 10MB)

**Dataclass de salida**:
```python
@dataclass
class DownloadResult:
    url: str
    html: Optional[str]
    final_url: str
    status_code: int
    download_status: str  # 'ok', 'error', 'blocked', 'timeout'
    error_message: str
    download_time: float  # segundos
```

---

### 3.2 `article_extractor.py`

**Responsabilidad**: Extraer texto principal del art√≠culo usando trafilatura.

**Funciones principales**:
- `extract_article_text(html: str, url: str) -> ExtractionResult`
  - Usa `trafilatura.extract()` con configuraci√≥n optimizada
  - Detecta idioma autom√°ticamente
  - Extrae metadatos (autor, fecha publicaci√≥n si disponible)

- `extract_with_fallback_bs4(html: str, url: str) -> Optional[str]`
  - Fallback con BeautifulSoup si trafilatura falla
  - Busca selectores comunes: `<article>`, `.article-body`, `.entry-content`, etc.
  - Selectores espec√≠ficos por dominio (elpais.com, elmundo.es, etc.)

**Configuraci√≥n trafilatura**:
```python
TRAFILATURA_CONFIG = {
    'include_comments': False,
    'include_tables': True,
    'include_images': False,
    'include_links': False,
    'output_format': 'txt',
    'favor_precision': True,  # Menos ruido, m√°s precisi√≥n
    'deduplicate': True
}

MIN_TEXT_LENGTH_OK = 200  # caracteres m√≠nimos para considerar OK
MIN_TEXT_LENGTH_WARNING = 100  # advertencia si es muy corto
```

**Selectores BeautifulSoup por dominio**:
```python
DOMAIN_SELECTORS = {
    'elpais.com': ['article.a_c', 'div.a_c_text', 'div.articulo-cuerpo'],
    'elmundo.es': ['article.ue-l-article__body', 'div.ue-c-article__body'],
    'abc.es': ['div.voc-article-content', 'div.cuerpo-texto'],
    'lavanguardia.com': ['div.article-modules', 'div.article-body'],
    'larazon.es': ['div.article-content', 'div.texto-noticia']
}
```

**Dataclass de salida**:
```python
@dataclass
class ExtractionResult:
    texto: str
    idioma: Optional[str]
    autor: Optional[str]
    fecha_publicacion: Optional[str]
    extraction_method: str  # 'trafilatura', 'bs4_fallback', 'playwright'
    extraction_status: str  # 'ok', 'no_contenido', 'error'
    char_count: int
    word_count: int
```

---

### 3.3 `article_cleaner.py`

**Responsabilidad**: Limpiar y normalizar texto extra√≠do.

**Funciones principales**:
- `clean_article_text(text: str) -> str`
  - Normalizaci√≥n Unicode (NFKC)
  - Eliminar scripts, estilos residuales
  - Eliminar fragmentos repetitivos comunes
  - Unificar saltos de l√≠nea
  - Trimming y saneamiento

**Patrones de limpieza**:
```python
REMOVE_PATTERNS = [
    r'Leer tambi√©n:.*?(?=\n|$)',
    r'Ver galer√≠a.*?(?=\n|$)',
    r'Relacionado:.*?(?=\n|$)',
    r'Suscr√≠bete.*?(?=\n|$)',
    r'M√°s informaci√≥n.*?(?=\n|$)',
    r'\[foto\]|\[v√≠deo\]|\[galer√≠a\]',
    r'Compartir en.*?(?=\n|$)',
    r'S√≠guenos en.*?(?=\n|$)'
]

MAX_CONSECUTIVE_NEWLINES = 2
MAX_CONSECUTIVE_SPACES = 1
```

**Normalizaci√≥n**:
- Convertir entidades HTML residuales
- Normalizar comillas tipogr√°ficas a ASCII
- Eliminar caracteres de control
- Unificar espacios en blanco
- Eliminar l√≠neas vac√≠as m√∫ltiples

---

### 3.4 `article_enricher.py`

**Responsabilidad**: Enriquecer metadatos del art√≠culo.

**Funciones principales**:
- `detect_language(text: str) -> str`
  - Usar detecci√≥n de trafilatura primero
  - Fallback a heur√≠stica simple (contar palabras espa√±olas comunes)

- `extract_metadata_from_html(html: str, url: str) -> dict`
  - Buscar metadatos en `<meta>` tags (Open Graph, Twitter Cards)
  - Extraer autor de selectores comunes
  - Extraer fecha de publicaci√≥n si no est√° en RSS

**Metadatos a extraer**:
```python
METADATA_FIELDS = {
    'og:title': 'titulo_og',
    'og:description': 'descripcion_og',
    'og:image': 'imagen_og',
    'article:author': 'autor',
    'article:published_time': 'fecha_publicacion',
    'article:section': 'seccion'
}
```

---

### 3.5 `article_fallback.py`

**Responsabilidad**: Fallback con Playwright para contenido din√°mico (JavaScript).

**Funciones principales**:
- `extract_with_playwright(url: str, timeout: int) -> Tuple[str, str]`
  - Lanza navegador headless
  - Espera carga completa (networkidle)
  - Extrae HTML renderizado
  - Retorna: `(html, screenshot_path)`

**Configuraci√≥n**:
```python
PLAYWRIGHT_ENABLED = False  # Desactivado por defecto
PLAYWRIGHT_TIMEOUT = 30000  # ms
PLAYWRIGHT_WAIT_FOR = 'networkidle'
PLAYWRIGHT_BROWSER = 'chromium'
PLAYWRIGHT_HEADLESS = True
MAX_PLAYWRIGHT_CALLS_PER_RUN = 10  # L√≠mite de seguridad
```

**Pol√≠tica de activaci√≥n**:
- Solo si `extraction_status == 'blocked'` o `'no_contenido'`
- Solo si dominio est√° en whitelist configurable
- Solo si no se ha superado l√≠mite de llamadas

**Whitelist de dominios**:
```python
PLAYWRIGHT_WHITELIST_DOMAINS = [
    # A√±adir solo dominios que requieren JS
    # Ejemplo: 'ejemplo-dinamico.com'
]
```

---

### 3.6 `article_processor.py`

**Responsabilidad**: Orquestador principal del flujo de extracci√≥n.

**Funciones principales**:
- `process_articles(input_file: str, config: dict) -> ProcessingReport`
  - Lee noticias desde `output.jsonl`
  - Orquesta descarga ‚Üí extracci√≥n ‚Üí limpieza ‚Üí enriquecimiento
  - Guarda resultados en `articles_full.jsonl` y `articles_full.csv`
  - Genera reporte de ejecuci√≥n

- `process_single_article(news_item: dict, config: dict) -> ArticleResult`
  - Procesa un art√≠culo individual
  - Maneja errores sin interrumpir flujo
  - Retorna resultado completo

**Modelo de datos de salida**:
```python
@dataclass
class ArticleResult:
    # Campos originales (del RSS)
    nombre_del_medio: str
    enlace: str
    titular: str
    fecha: str
    descripcion: str
    
    # Campos nuevos (extracci√≥n)
    texto: str
    idioma: str
    autor: Optional[str]
    fecha_publicacion: Optional[str]
    
    # Metadatos de extracci√≥n
    scrape_status: str  # 'ok', 'no_contenido_detectado', 'error_descarga', 
                        # 'error_parseo', 'blocked_fallback_required'
    error_message: str
    extraction_method: str  # 'trafilatura', 'bs4_fallback', 'playwright'
    char_count: int
    word_count: int
    download_time: float
    extraction_time: float
```

**Reporte de ejecuci√≥n**:
```python
@dataclass
class ProcessingReport:
    total_articles: int
    successful: int
    failed_download: int
    failed_extraction: int
    no_content: int
    blocked: int
    playwright_used: int
    
    total_time: float
    avg_time_per_article: float
    
    failed_urls: List[Tuple[str, str]]  # (url, reason)
    domains_needing_fallback: Set[str]
```

---

### 3.7 `main_extractor.py`

**Responsabilidad**: CLI para ejecutar extracci√≥n de art√≠culos.

**Interfaz CLI**:
```bash
python src/main_extractor.py \
    --input data/output.jsonl \
    --output data/articles_full.jsonl \
    --config config/extractor_config.yaml \
    --concurrency 5 \
    --enable-playwright \
    --log-level INFO
```

**Argumentos**:
- `--input`: Archivo JSONL con noticias filtradas (default: `data/output.jsonl`)
- `--output`: Archivo JSONL de salida (default: `data/articles_full.jsonl`)
- `--config`: Archivo de configuraci√≥n (default: `config/extractor_config.yaml`)
- `--concurrency`: Nivel de concurrencia (default: 5)
- `--enable-playwright`: Activar fallback Playwright (flag)
- `--log-level`: Nivel de logging (default: INFO)
- `--max-articles`: L√≠mite de art√≠culos a procesar (para testing)

**Salida**:
- `articles_full.jsonl`: Art√≠culos completos
- `articles_full.csv`: Art√≠culos completos en CSV
- `failed_extractions.jsonl`: URLs fallidas con raz√≥n
- `extraction_report.json`: Reporte de ejecuci√≥n

---

## 4. Configuraci√≥n

### 4.1 `config/extractor_config.yaml`

```yaml
# Configuraci√≥n del extractor de art√≠culos

downloader:
  timeout: 15
  max_retries: 3
  backoff_multiplier: 2
  delay_between_requests_same_domain: 1.0
  user_agent: "Mozilla/5.0 (compatible; RSSChinaBot-ArticleExtractor/1.0)"
  headers:
    Accept-Language: "es-ES,es;q=0.9,en;q=0.8"
    Accept: "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"

extractor:
  min_text_length_ok: 200
  min_text_length_warning: 100
  favor_precision: true
  include_tables: true
  include_comments: false

cleaner:
  max_consecutive_newlines: 2
  normalize_unicode: true
  remove_common_fragments: true

enricher:
  extract_metadata: true
  detect_language: true

fallback:
  playwright_enabled: false
  playwright_timeout: 30000
  playwright_browser: "chromium"
  playwright_headless: true
  max_playwright_calls_per_run: 10
  playwright_whitelist_domains: []

processing:
  concurrency: 5
  max_articles_per_run: null  # null = sin l√≠mite
  skip_already_processed: true

output:
  jsonl_path: "data/articles_full.jsonl"
  csv_path: "data/articles_full.csv"
  failed_path: "data/failed_extractions.jsonl"
  report_path: "data/extraction_report.json"
  csv_encoding: "utf-8-sig"  # UTF-8 con BOM para Excel

logging:
  log_file: "logs/article_extractor.log"
  log_level: "INFO"
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
```

---

## 5. Dependencias

### 5.1 `requirements.txt` (actualizado)

```txt
# Dependencias existentes
feedparser>=6.0.10
requests>=2.31.0
aiohttp>=3.9.0
beautifulsoup4>=4.12.0
lxml>=4.9.0
python-dateutil>=2.8.2
tenacity>=8.2.0
tqdm>=4.66.0
pydantic>=2.5.0

# Nuevas dependencias para extracci√≥n de art√≠culos
trafilatura>=1.6.0
playwright>=1.40.0
PyYAML>=6.0.1
langdetect>=1.0.9
```

### 5.2 Instalaci√≥n de Playwright

```bash
# Despu√©s de pip install playwright
playwright install chromium
```

---

## 6. Manejo de Errores y Robustez

### 6.1 Pol√≠tica de Errores

**Principio**: Nunca detener ejecuci√≥n completa por error en un art√≠culo.

**Estrategia**:
1. Capturar excepciones a nivel de art√≠culo individual
2. Registrar error con stack trace en logs
3. Marcar art√≠culo con `scrape_status` apropiado
4. Continuar con siguiente art√≠culo
5. Generar reporte de URLs fallidas al final

### 6.2 Clasificaci√≥n de Errores

| Error | scrape_status | Acci√≥n |
|-------|---------------|--------|
| Timeout descarga | `error_descarga` | Reintentar hasta MAX_RETRIES |
| HTTP 404/410 | `error_descarga` | No reintentar, marcar permanente |
| HTTP 5xx | `error_descarga` | Reintentar con backoff |
| Bloqueo detectado | `blocked_fallback_required` | Activar Playwright si enabled |
| Trafilatura devuelve None | `no_contenido_detectado` | Intentar BS4 fallback |
| Texto < MIN_LENGTH | `no_contenido_detectado` | Intentar BS4 fallback |
| Error en limpieza | `error_parseo` | Guardar texto sin limpiar |
| Error en Playwright | `blocked_fallback_required` | Marcar y continuar |

### 6.3 Reintentos con Tenacity

```python
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=2, min=2, max=30),
    retry=retry_if_exception_type((
        requests.Timeout,
        requests.ConnectionError,
        requests.HTTPError
    ))
)
def download_with_retry(url, timeout):
    # Implementaci√≥n
    pass
```

### 6.4 Rate Limiting por Dominio

```python
from collections import defaultdict
import time

class DomainRateLimiter:
    def __init__(self, delay: float):
        self.delay = delay
        self.last_request = defaultdict(float)
    
    def wait_if_needed(self, domain: str):
        elapsed = time.time() - self.last_request[domain]
        if elapsed < self.delay:
            time.sleep(self.delay - elapsed)
        self.last_request[domain] = time.time()
```

---

## 7. Concurrencia y Rendimiento

### 7.1 Estrategias de Concurrencia

**Opci√≥n 1: Secuencial** (`concurrency=1`)
- M√°s simple y seguro
- Adecuado para < 50 art√≠culos
- Tiempo estimado: ~5-10s por art√≠culo = 8-17 min para 100 art√≠culos

**Opci√≥n 2: ThreadPoolExecutor** (`concurrency=5`)
- Balance entre velocidad y cortes√≠a
- Adecuado para 50-500 art√≠culos
- Tiempo estimado: ~2-3s por art√≠culo = 3-5 min para 100 art√≠culos

**Opci√≥n 3: AsyncIO + aiohttp** (`concurrency=10`)
- M√°xima velocidad
- Requiere m√°s recursos
- Adecuado para > 500 art√≠culos
- Tiempo estimado: ~1-2s por art√≠culo = 2-3 min para 100 art√≠culos

### 7.2 Implementaci√≥n Recomendada

**Para el proyecto actual**: ThreadPoolExecutor con `concurrency=5`

```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def process_articles_concurrent(articles, config):
    concurrency = config['processing']['concurrency']
    results = []
    
    with ThreadPoolExecutor(max_workers=concurrency) as executor:
        futures = {
            executor.submit(process_single_article, article, config): article
            for article in articles
        }
        
        for future in tqdm(as_completed(futures), total=len(articles)):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                logger.error(f"Error processing article: {e}")
    
    return results
```

### 7.3 L√≠mites y Recomendaciones

- **Concurrency recomendada**: 5 (balance √≥ptimo)
- **Delay por dominio**: 1.0s (respetuoso con servidores)
- **Timeout por art√≠culo**: 15s (suficiente para mayor√≠a de casos)
- **Volumen diario recomendado**: 100-500 art√≠culos
- **Tiempo ejecuci√≥n estimado**: 5-15 minutos para 100 art√≠culos

---

## 8. Logging y Monitoreo

### 8.1 Configuraci√≥n de Logging

```python
import logging
from pathlib import Path

def setup_logging(log_file: str, log_level: str):
    Path(log_file).parent.mkdir(parents=True, exist_ok=True)
    
    logging.basicConfig(
        level=getattr(logging, log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
```

### 8.2 Eventos a Registrar

**INFO**:
- Inicio/fin de ejecuci√≥n
- N√∫mero de art√≠culos a procesar
- Progreso cada 10 art√≠culos
- Resumen final

**WARNING**:
- Texto extra√≠do muy corto (< MIN_LENGTH_WARNING)
- Bloqueo detectado
- Fallback activado
- Reintentos

**ERROR**:
- Errores de descarga permanentes
- Errores de extracci√≥n
- Excepciones no esperadas

### 8.3 M√©tricas a Recopilar

```python
@dataclass
class ExecutionMetrics:
    start_time: datetime
    end_time: datetime
    total_articles: int
    
    # Por status
    status_counts: Dict[str, int]
    
    # Por m√©todo de extracci√≥n
    extraction_method_counts: Dict[str, int]
    
    # Tiempos
    total_download_time: float
    total_extraction_time: float
    avg_time_per_article: float
    
    # Dominios problem√°ticos
    domains_with_errors: Dict[str, int]
    domains_needing_playwright: Set[str]
```

### 8.4 Reporte Final

```json
{
  "execution_summary": {
    "start_time": "2025-11-26T18:00:00",
    "end_time": "2025-11-26T18:12:34",
    "duration_seconds": 754,
    "total_articles": 100
  },
  "results": {
    "successful": 87,
    "failed_download": 5,
    "failed_extraction": 3,
    "no_content": 4,
    "blocked": 1
  },
  "extraction_methods": {
    "trafilatura": 82,
    "bs4_fallback": 5,
    "playwright": 0
  },
  "performance": {
    "avg_time_per_article": 7.54,
    "total_download_time": 423.2,
    "total_extraction_time": 89.3
  },
  "problematic_domains": {
    "ejemplo.com": {
      "errors": 3,
      "needs_playwright": false
    }
  },
  "failed_urls": [
    {
      "url": "https://ejemplo.com/articulo",
      "reason": "HTTP 404",
      "status": "error_descarga"
    }
  ]
}
```

---

## 9. Testing y Validaci√≥n

### 9.1 Tests Unitarios

**`tests/test_article_extractor/test_downloader.py`**:
```python
def test_download_success():
    # Mock requests.get con respuesta 200
    # Verificar que devuelve HTML correcto

def test_download_404():
    # Mock requests.get con respuesta 404
    # Verificar que marca error_descarga

def test_download_timeout():
    # Mock timeout
    # Verificar reintentos

def test_download_blocked():
    # Mock respuesta con captcha
    # Verificar detecci√≥n de bloqueo
```

**`tests/test_article_extractor/test_extractor.py`**:
```python
def test_extract_with_trafilatura():
    # HTML con art√≠culo bien formado
    # Verificar extracci√≥n correcta

def test_extract_no_content():
    # HTML sin contenido de art√≠culo
    # Verificar que devuelve no_contenido

def test_extract_bs4_fallback():
    # HTML que trafilatura no puede parsear
    # Verificar que BS4 fallback funciona
```

**`tests/test_article_extractor/test_cleaner.py`**:
```python
def test_clean_unicode():
    # Texto con caracteres especiales
    # Verificar normalizaci√≥n

def test_remove_fragments():
    # Texto con "Leer tambi√©n:", etc.
    # Verificar eliminaci√≥n

def test_normalize_whitespace():
    # Texto con m√∫ltiples saltos de l√≠nea
    # Verificar unificaci√≥n
```

### 9.2 Tests de Integraci√≥n

**Escenario 1: Art√≠culo normal**
```python
def test_integration_normal_article():
    # URL de art√≠culo real (El Pa√≠s)
    # Verificar flujo completo: descarga ‚Üí extracci√≥n ‚Üí limpieza
    # Verificar que scrape_status == 'ok'
    # Verificar que texto tiene > MIN_LENGTH
```

**Escenario 2: Art√≠culo bloqueado**
```python
def test_integration_blocked_article():
    # URL que requiere JS (si existe)
    # Verificar que detecta bloqueo
    # Verificar que marca blocked_fallback_required
```

**Escenario 3: Art√≠culo sin contenido**
```python
def test_integration_no_content():
    # URL v√°lida pero sin art√≠culo (p√°gina de categor√≠a)
    # Verificar que marca no_contenido_detectado
```

### 9.3 Datos de Prueba

Crear archivo `tests/test_data/sample_articles.json`:
```json
[
  {
    "nombre_del_medio": "El Pa√≠s",
    "enlace": "https://elpais.com/internacional/...",
    "titular": "Ejemplo de titular",
    "fecha": "2025-11-26T18:00:00",
    "descripcion": "Descripci√≥n de prueba"
  }
]
```

### 9.4 Validaci√≥n Manual

**Checklist de validaci√≥n**:
- [ ] Ejecutar con 10 art√≠culos reales
- [ ] Verificar que todos tienen `scrape_status`
- [ ] Revisar calidad de texto extra√≠do (sin ruido)
- [ ] Verificar que CSV es legible en Excel
- [ ] Verificar que JSONL es v√°lido
- [ ] Revisar logs para errores
- [ ] Verificar reporte de ejecuci√≥n

---

## 10. Integraci√≥n con GUI

### 10.1 Modificaciones en `gui.py`

**A√±adir bot√≥n en toolbar**:
```python
tk.Button(results_toolbar, text="üìù Extraer Texto Completo",
         command=self.extract_full_articles,
         bg=self.colors['warning'], fg='white',
         font=('Segoe UI', 9, 'bold'),
         relief='flat', padx=15, pady=8,
         cursor='hand2').pack(side=tk.LEFT, padx=5)
```

**A√±adir m√©todo**:
```python
def extract_full_articles(self):
    """Ejecuta extracci√≥n de texto completo en thread separado."""
    if not Path(self.output_dir.get() + "/output.jsonl").exists():
        messagebox.showwarning("Sin datos",
                             "Primero ejecuta el filtrado de noticias.")
        return
    
    # Confirmar
    if not messagebox.askyesno("Confirmar",
                              "¬øExtraer texto completo de los art√≠culos?\n"
                              "Esto puede tardar varios minutos."):
        return
    
    # Ejecutar en thread
    thread = threading.Thread(target=self.run_article_extraction, daemon=True)
    thread.start()

def run_article_extraction(self):
    """Ejecuta extracci√≥n (en thread separado)."""
    from article_processor import process_articles
    
    try:
        config = load_config('config/extractor_config.yaml')
        report = process_articles(
            input_file=self.output_dir.get() + "/output.jsonl",
            config=config
        )
        
        self.root.after(0, lambda: messagebox.showinfo(
            "Extracci√≥n completada",
            f"Art√≠culos procesados: {report.total_articles}\n"
            f"Exitosos: {report.successful}\n"
            f"Fallidos: {report.total_articles - report.successful}"
        ))
    except Exception as e:
        logger.error(f"Error en extracci√≥n: {e}", exc_info=True)
        self.root.after(0, lambda: messagebox.showerror(
            "Error", f"Error durante extracci√≥n:\n{str(e)}"
        ))
```

### 10.2 Nueva Tab en Notebook

**A√±adir tab "Art√≠culos Completos"**:
```python
# Tab 3: Art√≠culos Completos
articles_frame = tk.Frame(self.notebook, bg='white')
self.notebook.add(articles_frame, text='üìù Art√≠culos Completos')

# Viewer de texto completo
# Similar a tab de resultados pero con preview de texto
```

---

## 11. Despliegue y Operaci√≥n

### 11.1 Instalaci√≥n

```bash
# 1. Activar entorno virtual
cd f:/pautalla/china
.venv\Scripts\activate

# 2. Instalar nuevas dependencias
pip install -r requirements.txt

# 3. Instalar navegador Playwright (solo si se usa fallback)
playwright install chromium

# 4. Verificar instalaci√≥n
python src/main_extractor.py --help
```

### 11.2 Ejecuci√≥n Manual

```bash
# Extracci√≥n b√°sica
python src/main_extractor.py

# Con configuraci√≥n personalizada
python src/main_extractor.py \
    --input data/output.jsonl \
    --output data/articles_full.jsonl \
    --concurrency 5 \
    --log-level INFO

# Con Playwright activado
python src/main_extractor.py --enable-playwright
```

### 11.3 Ejecuci√≥n Programada (Cron)

**Windows Task Scheduler**:
```batch
@echo off
cd f:\pautalla\china
call .venv\Scripts\activate
python src/main_extractor.py --log-level INFO
```

**Frecuencia recomendada**: Diaria, despu√©s de ejecutar filtrado RSS

### 11.4 Rotaci√≥n de Logs

```python
from logging.handlers import RotatingFileHandler

handler = RotatingFileHandler(
    'logs/article_extractor.log',
    maxBytes=10*1024*1024,  # 10 MB
    backupCount=5,
    encoding='utf-8'
)
```

### 11.5 Checklist de Despliegue Diario

- [ ] Verificar que `output.jsonl` tiene noticias nuevas
- [ ] Ejecutar `main_extractor.py`
- [ ] Revisar `extraction_report.json`
- [ ] Verificar que `articles_full.jsonl` se actualiz√≥
- [ ] Revisar `failed_extractions.jsonl` para URLs problem√°ticas
- [ ] Revisar logs para errores
- [ ] Backup de datos (opcional)

---

## 12. Estrategia de Fallback con Playwright

### 12.1 Cu√°ndo Activar Playwright

**Activar solo si**:
1. `scrape_status == 'blocked_fallback_required'` o `'no_contenido_detectado'`
2. Dominio est√° en whitelist configurable
3. No se ha superado l√≠mite de llamadas (`MAX_PLAYWRIGHT_CALLS_PER_RUN`)

**No activar si**:
- Error de red (timeout, connection error)
- HTTP 404/410 (recurso no existe)
- Texto extra√≠do es suficiente (> MIN_LENGTH)

### 12.2 Whitelist de Dominios

Inicialmente vac√≠a. A√±adir dominios solo despu√©s de verificar que:
1. Requieren JavaScript para cargar contenido
2. No tienen API alternativa
3. Son fuentes importantes

```yaml
fallback:
  playwright_whitelist_domains:
    # - "ejemplo-dinamico.com"
```

### 12.3 L√≠mites Operativos

- **M√°ximo llamadas por ejecuci√≥n**: 10 (configurable)
- **Timeout por p√°gina**: 30s
- **Navegador**: Chromium headless
- **Recursos**: ~200MB RAM por instancia

### 12.4 Monitoreo de Playwright

Registrar en logs:
- N√∫mero de veces activado
- Dominios que lo requieren
- Tiempo de ejecuci√≥n
- √âxito/fallo

Si un dominio requiere Playwright frecuentemente:
1. Investigar si hay selector BS4 espec√≠fico
2. Considerar a√±adir a whitelist permanente
3. Evaluar si vale la pena el overhead

---

## 13. M√©tricas de √âxito

### 13.1 KPIs del Sistema

| M√©trica | Objetivo | Cr√≠tico si |
|---------|----------|------------|
| Tasa de √©xito | > 85% | < 70% |
| Tiempo promedio/art√≠culo | < 10s | > 20s |
| Art√≠culos con texto completo | > 80% | < 60% |
| Uso de Playwright | < 5% | > 20% |
| Errores de descarga | < 10% | > 25% |

### 13.2 Calidad del Texto Extra√≠do

**Criterios de calidad**:
- Longitud > 200 caracteres
- Sin fragmentos de men√∫/navegaci√≥n
- Sin scripts/estilos residuales
- P√°rrafos coherentes
- Idioma detectado correctamente

**Validaci√≥n manual**: Revisar 10 art√≠culos aleatorios semanalmente

---

## 14. Roadmap de Implementaci√≥n

### Fase 1: Core (Semana 1)
- [ ] Implementar `article_downloader.py`
- [ ] Implementar `article_extractor.py` (solo trafilatura)
- [ ] Implementar `article_cleaner.py`
- [ ] Implementar `article_processor.py` (b√°sico)
- [ ] Tests unitarios b√°sicos
- [ ] Ejecutar con 10 art√≠culos de prueba

### Fase 2: Robustez (Semana 2)
- [ ] A√±adir fallback BS4 en `article_extractor.py`
- [ ] Implementar `article_enricher.py`
- [ ] Mejorar manejo de errores
- [ ] A√±adir rate limiting por dominio
- [ ] Tests de integraci√≥n
- [ ] Ejecutar con 100 art√≠culos reales

### Fase 3: Fallback Playwright (Semana 3)
- [ ] Implementar `article_fallback.py`
- [ ] Configurar whitelist de dominios
- [ ] Tests con sitios din√°micos
- [ ] Optimizar rendimiento

### Fase 4: CLI y Configuraci√≥n (Semana 4)
- [ ] Implementar `main_extractor.py`
- [ ] Crear `extractor_config.yaml`
- [ ] Documentaci√≥n completa
- [ ] Integraci√≥n con GUI
- [ ] Despliegue en producci√≥n

---

## 15. Consideraciones Finales

### 15.1 Limitaciones Conocidas

- **JavaScript pesado**: Algunos sitios requieren Playwright (overhead)
- **Paywalls**: Art√≠culos de pago no son accesibles
- **Rate limiting**: Algunos medios pueden bloquear si se excede l√≠mite
- **Cambios en estructura**: Selectores BS4 pueden quedar obsoletos

### 15.2 Mejoras Futuras

- Cache de art√≠culos ya procesados (evitar re-descarga)
- Detecci√≥n autom√°tica de selectores por dominio
- Integraci√≥n con base de datos (SQLite/PostgreSQL)
- API REST para consultar art√≠culos
- Dashboard web para monitoreo

### 15.3 Mantenimiento

**Mensual**:
- Revisar dominios con alta tasa de fallo
- Actualizar selectores BS4 si es necesario
- Revisar y limpiar logs antiguos

**Trimestral**:
- Actualizar dependencias (pip)
- Revisar y optimizar configuraci√≥n
- Evaluar m√©tricas de calidad

---

## Ap√©ndices

### A. Ejemplo de Art√≠culo Procesado

```json
{
  "nombre_del_medio": "El Pa√≠s",
  "enlace": "https://elpais.com/internacional/2025-11-26/china-anuncia-nuevas-medidas.html",
  "titular": "China anuncia nuevas medidas econ√≥micas",
  "fecha": "2025-11-26T10:30:00+00:00",
  "descripcion": "El gobierno chino presenta un paquete de est√≠mulos...",
  "texto": "El gobierno de China anunci√≥ este martes un nuevo paquete de medidas econ√≥micas destinadas a impulsar el crecimiento...\n\nLas autoridades econ√≥micas del pa√≠s asi√°tico...",
  "idioma": "es",
  "autor": "Juan P√©rez",
  "fecha_publicacion": "2025-11-26T10:00:00+00:00",
  "scrape_status": "ok",
  "error_message": "",
  "extraction_method": "trafilatura",
  "char_count": 3542,
  "word_count": 587,
  "download_time": 2.34,
  "extraction_time": 0.12
}
```

### B. Ejemplo de URL Fallida

```json
{
  "url": "https://ejemplo.com/articulo-bloqueado",
  "nombre_del_medio": "Ejemplo",
  "titular": "Art√≠culo de prueba",
  "scrape_status": "blocked_fallback_required",
  "error_message": "Captcha detected in HTML response",
  "timestamp": "2025-11-26T18:15:32",
  "http_status": 200,
  "attempts": 3
}
```

### C. Comandos √ötiles

```bash
# Ver estad√≠sticas de art√≠culos procesados
python -c "import json; data=[json.loads(l) for l in open('data/articles_full.jsonl')]; print(f'Total: {len(data)}'); print(f'OK: {sum(1 for d in data if d[\"scrape_status\"]==\"ok\")}')"

# Listar dominios con m√°s errores
grep "error_descarga" data/articles_full.jsonl | jq -r '.enlace' | sed 's|https\?://||' | cut -d/ -f1 | sort | uniq -c | sort -rn

# Ver art√≠culos m√°s largos
jq -r 'select(.scrape_status=="ok") | "\(.char_count)\t\(.titular)"' data/articles_full.jsonl | sort -rn | head -10
```

---

**Fin de la Especificaci√≥n T√©cnica**

**Versi√≥n**: 1.0  
**Fecha**: 2025-11-26  
**Autor**: Especificaci√≥n para RSS China News Filter - Article Extractor Module
